{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as T\n",
    "from model import Generator, Discriminator\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from math import log2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train_at_image_size = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "lr = 3e-4\n",
    "batch_sizes = [16, 16, 16, 16, 16, 16, 16, 8, 4]\n",
    "image_size = 512\n",
    "channels_img = 3\n",
    "z_dim = 256\n",
    "in_channels = 256\n",
    "lambda_gp = 10\n",
    "steps = int(log2(image_size/4)) + 1\n",
    "\n",
    "progressive_epochs = [20] * len(batch_sizes)\n",
    "fixed_noise = torch.randn(8, z_dim, 1, 1).to(device)\n",
    "num_workers = 4\n",
    "\n",
    "torch.backends.cudnn.benchmarks = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(discriminator, real, fake, alpha, train_step, device='cpu'):\n",
    "    batch_size, c, h, w = real.shape\n",
    "    beta = torch.rand((batch_size, 1, 1, 1)).repeat(1, c, h, w).to(device)\n",
    "\n",
    "    interpolated_images = real * beta + fake.detach() * (1 - beta)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "\n",
    "    mixed_scores = discriminator(interpolated_images, alpha, train_step)\n",
    "\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_to_tensorboard(writer, loss_disc, loss_gen, real, fake, tensorboard_step):\n",
    "    writer.add_scalar(\"Loss Discriminator\", loss_disc, global_step=tensorboard_step)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img_grid_real = torchvision.utils.make_grid(real[:8], normalize=True)\n",
    "        img_grid_fake = torchvision.utils.make_grid(fake[:8], normalize=True)\n",
    "        writer.add_image(\"Real\", img_grid_real, global_step=tensorboard_step)\n",
    "        writer.add_image(\"Fake\", img_grid_fake, global_step=tensorboard_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(image_size):\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.ToTensor(),\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.Normalize(\n",
    "                [0.5 for _ in range(channels_img)],\n",
    "                [0.5 for _ in range(channels_img)],\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    batch_size = batch_sizes[int(log2(image_size / 4))]\n",
    "    dataset = ImageFolder(root='./dataset', transform=transform)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    return loader, dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(disc, gen, loader, dataset, step, alpha, opt_disc, opt_gen, tensorboard_step, writer, scaler_gen, scaler_disc):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch_idx, (real, _) in enumerate(loop):\n",
    "        real = real.to(device)\n",
    "        current_batch_size = real.shape[0]\n",
    "\n",
    "        noise = torch.randn(current_batch_size, z_dim, 1, 1).to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            fake = gen(noise, alpha, step)\n",
    "            disc_real = disc(real, alpha, step)\n",
    "            disc_fake = disc(fake.detach(), alpha, step)\n",
    "\n",
    "            gp = gradient_penalty(disc, real, fake, alpha, step, device=device)\n",
    "            loss_disc = (\n",
    "                -(torch.mean(disc_real) - torch.mean(disc_fake))\n",
    "                + lambda_gp * gp\n",
    "                + (0.001 * torch.mean(disc_real ** 2))\n",
    "            )\n",
    "\n",
    "        opt_disc.zero_grad()\n",
    "        scaler_disc.scale(loss_disc).backward()\n",
    "        scaler_disc.step(opt_disc)\n",
    "        scaler_disc.update()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            gen_fake = disc(fake, alpha, step)\n",
    "            loss_gen = -torch.mean(gen_fake)\n",
    "        \n",
    "        opt_gen.zero_grad()\n",
    "        scaler_gen.scale(loss_gen).backward()\n",
    "        scaler_gen.step(opt_gen)\n",
    "        scaler_gen.update()\n",
    "\n",
    "        alpha += current_batch_size / (len(dataset) * (progressive_epochs[step]*0.5))\n",
    "        alpha = min(alpha, 1)\n",
    "\n",
    "        if batch_idx % 500 == 0:\n",
    "            with torch.no_grad():\n",
    "                fixed_fakes = gen(fixed_noise, alpha, step) * 0.5 + 0.5\n",
    "            plot_to_tensorboard(\n",
    "                writer,\n",
    "                loss_disc.item(),\n",
    "                loss_gen.item(),\n",
    "                real.detach(),\n",
    "                fixed_fakes.detach(),\n",
    "                tensorboard_step\n",
    "            )\n",
    "            tensorboard_step += 1\n",
    "        \n",
    "    return tensorboard_step, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    gen = Generator(z_dim, in_channels, channels_img).to(device)\n",
    "    disc = Discriminator(in_channels, channels_img).to(device)\n",
    "\n",
    "    opt_gen = torch.optim.Adam(gen.parameters(), lr=lr, betas=(0.0, 0.99))\n",
    "    opt_disc = torch.optim.Adam(disc.parameters(), lr=lr, betas=(0.0, 0.99))\n",
    "\n",
    "    scaler_disc = torch.cuda.amp.GradScaler()\n",
    "    scaler_gen = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    writer = SummaryWriter(f'logs/gan')\n",
    "\n",
    "    tensorboard_step = 0\n",
    "    step = int(log2(start_train_at_image_size / 4))\n",
    "    for num_epochs in progressive_epochs[step:]:\n",
    "        alpha = 1e-5\n",
    "        loader, dataset = get_loader(4*2**step)\n",
    "        print(f\"Размер изображения {4*2**step}\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Эпоха: {epoch}/{num_epochs}')\n",
    "            tensorboard_step, alpha = train_func(disc, gen, loader, dataset, step, alpha, opt_disc, opt_gen, tensorboard_step, writer, scaler_gen, scaler_disc)\n",
    "\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
